{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":101408,"databundleVersionId":12268213,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Check and Read Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n\ntrain_captions = pd.read_csv('/kaggle/input/obss-intern-competition-2025/train.csv')\ntest_captions = pd.read_csv('/kaggle/input/obss-intern-competition-2025/test.csv')\nimg_dir = '/kaggle/input/obss-intern-competition-2025/train/train'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:45:00.728628Z","iopub.execute_input":"2025-05-25T19:45:00.728880Z","iopub.status.idle":"2025-05-25T19:45:04.342704Z","shell.execute_reply.started":"2025-05-25T19:45:00.728856Z","shell.execute_reply":"2025-05-25T19:45:04.341835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_captions.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:45:42.454248Z","iopub.execute_input":"2025-05-25T19:45:42.454960Z","iopub.status.idle":"2025-05-25T19:45:42.467331Z","shell.execute_reply.started":"2025-05-25T19:45:42.454931Z","shell.execute_reply":"2025-05-25T19:45:42.466560Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Clean Caption Data","metadata":{}},{"cell_type":"code","source":"import re\nfrom collections import Counter\n\n\ndef remove_punc(text) -> str:\n    return re.sub(r'[^\\w\\s]','',text)\n\ndef to_lower_case(text) -> str:\n    return text.lower()\n\n\n\ndef remove_numbers(text) -> str:\n    return re.sub(r'[0-9]','',text)\n\ndef remove_multiple_spaces(text) -> str:\n    return re.sub(r' +',' ',text).strip()\n\n\n\ndef clean_text(text) -> str:\n    text = remove_punc(text)\n    text = to_lower_case(text)\n   # text = remove_stopwords(text)\n    text = remove_numbers(text)\n    text = remove_multiple_spaces(text)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:45:44.911354Z","iopub.execute_input":"2025-05-25T19:45:44.911967Z","iopub.status.idle":"2025-05-25T19:45:44.919330Z","shell.execute_reply.started":"2025-05-25T19:45:44.911936Z","shell.execute_reply":"2025-05-25T19:45:44.918442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_captions['caption'] = train_captions['caption'].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:45:46.826376Z","iopub.execute_input":"2025-05-25T19:45:46.826928Z","iopub.status.idle":"2025-05-25T19:45:47.060935Z","shell.execute_reply.started":"2025-05-25T19:45:46.826897Z","shell.execute_reply":"2025-05-25T19:45:47.060201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"caption_lengths = train_captions['caption'].apply(lambda x: len(str(x).split()))\n\nprint(caption_lengths.describe())\n\nplt.figure(figsize=(10, 6))\nplt.hist(caption_lengths, bins=50, alpha=0.7, color='blue')\nplt.title('Distribution of Explanation Lengths')\nplt.xlabel('Word Count')\nplt.ylabel('Number of Descriptions')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:47:00.920471Z","iopub.execute_input":"2025-05-25T19:47:00.921078Z","iopub.status.idle":"2025-05-25T19:47:01.246706Z","shell.execute_reply.started":"2025-05-25T19:47:00.921052Z","shell.execute_reply":"2025-05-25T19:47:01.245955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image\nfrom textwrap import wrap\n\ndef read_image(path,img_size=224):\n    transform = transforms.Compose([\n        transforms.Resize((img_size, img_size)),\n        transforms.ToTensor()  \n    ])\n    image = Image.open(path).convert(\"RGB\")\n    return transform(image)\n\ndef display_images(temp_df, img_path):\n    temp_df = temp_df.reset_index(drop=True)\n    plt.figure(figsize=(20, 20))\n    for i in range(min(15, len(temp_df))):\n        plt.subplot(5, 5, i + 1)\n        plt.subplots_adjust(hspace=0.9, wspace=0.5)\n        \n        image_filename = str(temp_df.image_id[i])\n        if not image_filename.endswith('.jpg'):\n            image_filename += '.jpg'\n        \n        image_path = os.path.join(img_path, image_filename)\n        image_tensor = read_image(image_path)\n        image_np = image_tensor.permute(1, 2, 0).numpy()\n        plt.imshow(image_np)\n        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n        plt.axis(\"off\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:38:14.462240Z","iopub.execute_input":"2025-05-25T18:38:14.462506Z","iopub.status.idle":"2025-05-25T18:38:14.469197Z","shell.execute_reply.started":"2025-05-25T18:38:14.462486Z","shell.execute_reply":"2025-05-25T18:38:14.468373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_images(train_captions.sample(15),img_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T12:29:58.247458Z","iopub.execute_input":"2025-05-17T12:29:58.248081Z","iopub.status.idle":"2025-05-17T12:30:00.104168Z","shell.execute_reply.started":"2025-05-17T12:29:58.248056Z","shell.execute_reply":"2025-05-17T12:30:00.102765Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Models Definition","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision.models import ResNet50_Weights\nfrom PIL import Image\nimport math\n\nclass EncoderCNN(nn.Module):\n    def __init__(self, embed_size, dropout_p=0.5): \n        super(EncoderCNN, self).__init__()\n        weights = ResNet50_Weights.DEFAULT\n        resnet = models.resnet50(weights=weights)\n        \n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        \n        self.adaptive_pool = nn.AdaptiveAvgPool2d((14, 14))\n        \n        self.conv_to_embed = nn.Conv2d(2048, embed_size, kernel_size=1)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout_p) \n        \n        self.layer_norm = nn.LayerNorm(embed_size)\n        \n    def forward(self, images):\n        features = self.resnet(images)  \n        features = self.adaptive_pool(features)  \n        features = self.conv_to_embed(features)  \n        features = self.relu(features)\n        features = self.dropout(features) \n        \n        features = features.permute(0, 2, 3, 1)\n        features = self.layer_norm(features)\n        \n        return features\n\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout_p=0.1, max_len=5000): \n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout_p) \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  \n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x) \n\nclass DecoderTransformer(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, num_heads, dropout_p): \n        super(DecoderTransformer, self).__init__()\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        \n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.positional_encoding = PositionalEncoding(embed_size, dropout_p=dropout_p) \n        \n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=embed_size,  \n            nhead=num_heads, \n            dim_feedforward=hidden_size,  \n            dropout=dropout_p, \n            batch_first=True\n        )\n        \n        self.transformer_decoder = nn.TransformerDecoder(\n            decoder_layer,  \n            num_layers=num_layers\n        )\n        \n        self.fc_out = nn.Linear(embed_size, vocab_size)\n        self.dropout = nn.Dropout(dropout_p) \n        \n        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n\n    def _generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask.to(torch.bool)\n\n    def forward(self, encoder_features, captions):\n        batch_size, seq_len = captions.size()\n        \n        memory = encoder_features.view(batch_size, -1, self.embed_size)\n        \n        tgt = self.embedding(captions)\n        tgt = self.positional_encoding(tgt)\n        \n        \n        tgt_mask = self._generate_square_subsequent_mask(seq_len).to(tgt.device)\n        \n        output = self.transformer_decoder(\n            tgt,  \n            memory,  \n            tgt_mask=tgt_mask,\n        )\n        \n        output = self.dropout(output) \n        outputs = self.fc_out(output)\n        \n        return outputs, None \n\n    def caption_image(self, image, vocabulary, encoder_cnn, max_length=26, device='cuda'):\n        self.eval()\n        encoder_cnn.eval()\n        \n        with torch.no_grad():\n            encoder_out = encoder_cnn(image)\n            memory = encoder_out.view(1, -1, self.embed_size)\n            \n            sos_token_id = vocabulary.stoi.get(\"<SOS>\", vocabulary.stoi.get(\"<SOS>\", 1))\n            \n            generated_ids = [sos_token_id]\n            result_caption = []\n            attention_maps = [] \n            \n            for step in range(max_length):\n                current_sequence_tensor = torch.tensor(generated_ids, device=device).unsqueeze(0)\n                \n                tgt_embed = self.embedding(current_sequence_tensor)\n                tgt_embed = self.positional_encoding(tgt_embed)\n                \n                tgt_mask = self._generate_square_subsequent_mask(current_sequence_tensor.size(1)).to(device)\n                \n                decoder_output = self.transformer_decoder(\n                    tgt_embed,  \n                    memory,  \n                    tgt_mask=tgt_mask\n                )\n                \n                last_token_output = decoder_output[:, -1, :]\n                \n                output = self.fc_out(last_token_output)\n                predicted_id = output.argmax(1).item()\n                \n                predicted_word = vocabulary.itos[predicted_id]\n                \n                if predicted_word in [\"<EOS>\"]:\n                    break\n                    \n                if predicted_word not in [\"<SOS>\"]:\n                    result_caption.append(predicted_word)\n                    \n                generated_ids.append(predicted_id)\n        \n        return result_caption, attention_maps\n\nclass ImageCaption(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, num_heads, dropout):\n        super(ImageCaption, self).__init__()\n        self.encoder_cnn = EncoderCNN(embed_size, dropout_p=dropout) \n        self.decoder_transformer = DecoderTransformer(\n            embed_size=embed_size,\n            hidden_size=hidden_size,  \n            vocab_size=vocab_size,\n            num_layers=num_layers,\n            num_heads=num_heads, \n            dropout_p=dropout \n        )\n\n    def forward(self, images, captions):\n        features = self.encoder_cnn(images)\n        outputs, _ = self.decoder_transformer(features, captions)  \n        return outputs, None\n\n    def caption_image(self, image, vocabulary, max_length=26, device='cuda'):\n        return self.decoder_transformer.caption_image(\n            image, vocabulary, self.encoder_cnn, max_length, device\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:55:15.266388Z","iopub.execute_input":"2025-05-25T07:55:15.266680Z","iopub.status.idle":"2025-05-25T07:55:22.801169Z","shell.execute_reply.started":"2025-05-25T07:55:15.266659Z","shell.execute_reply":"2025-05-25T07:55:22.800434Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset preparation","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom torchvision import transforms\nfrom PIL import Image\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize(256),                  \n    transforms.CenterCrop(224),               \n\n    transforms.RandomHorizontalFlip(p=0.5),  \n    transforms.RandomRotation(degrees=5),     \n\n    transforms.ColorJitter(\n        brightness=0.1,\n        contrast=0.1,\n        saturation=0.1,\n        hue=0.05\n    ),\n\n    transforms.RandomApply([\n        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))\n    ], p=0.1),\n\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n\nval_transforms = transforms.Compose([\n     transforms.Resize(256),         \n    transforms.CenterCrop(224),     \n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n\nclass CaptionDataset(Dataset):\n    def __init__(self, dataframe, img_dir, word2idx, max_len=50, transform=None):\n        self.dataframe = dataframe.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.word2idx = word2idx\n        self.max_len = max_len\n        self.transform = transform \n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_path = os.path.join(self.img_dir, f\"{row['image_id']}.jpg\")\n\n        image = Image.open(image_path).convert(\"RGB\")\n\n        if self.transform is not None:\n            image = self.transform(image)\n\n        caption = row['caption'].lower().split()\n        caption = ['<SOS>'] + caption[:self.max_len - 2] + ['<EOS>']\n\n        caption_ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in caption]\n\n        caption_ids += [self.word2idx['<PAD>']] * (self.max_len - len(caption_ids))\n\n        return image, torch.tensor(caption_ids)\n\n\nclass Vocabulary:\n    def __init__(self, freq_threshold=5):\n        self.itos = {0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>'}\n        self.stoi = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n        self.freq_threshold = freq_threshold\n        self.index = 4\n\n    def __len__(self):\n        return len(self.itos)\n\n    def build_vocabulary(self, caption_series):\n        word_counts = {}\n        for caption in caption_series:\n            for word in caption.lower().split():\n                if word not in word_counts:\n                    word_counts[word] = 1\n                else:\n                    word_counts[word] += 1\n\n        for word, count in word_counts.items():\n            if count >= self.freq_threshold and word not in self.stoi:\n                self.stoi[word] = self.index\n                self.itos[self.index] = word\n                self.index += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:56:34.302460Z","iopub.execute_input":"2025-05-25T07:56:34.303093Z","iopub.status.idle":"2025-05-25T07:56:34.314423Z","shell.execute_reply.started":"2025-05-25T07:56:34.303071Z","shell.execute_reply":"2025-05-25T07:56:34.313544Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, random_split \nfrom sklearn.model_selection import train_test_split \nfrom torch.optim import AdamW\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nimport time\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n\nvocab = Vocabulary()\nvocab.build_vocabulary(train_captions['caption']) \n\n\nembed_size = 512\nhidden_size = 2048\nnum_heads = 8\nnum_layers = 8\ndropout = 0.4               \n\nbatch_size = 64\nnum_epochs = 500            \nlearning_rate = 1e-4      \n#warmup_steps = 1000       \nweight_decay = 1e-3       \nlabel_smoothing = 0.05     \nmax_norm = 1.0            \n\n\n\ntrain_df, val_df = train_test_split(train_captions, test_size=0.1, random_state=42) \n\n\ntrain_dataset = CaptionDataset(train_df, img_dir, vocab.stoi, transform=train_transforms)\nval_dataset = CaptionDataset(val_df, img_dir, vocab.stoi, transform=val_transforms)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4) \n\nprint(f\"Eğitim seti boyutu: {len(train_dataset)}\")\nprint(f\"Doğrulama seti boyutu: {len(val_dataset)}\")\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name()}\")\n    print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n\n\n\nmodel = ImageCaption(\n    embed_size=embed_size,\n    hidden_size=hidden_size,\n    vocab_size=len(vocab),\n    num_layers=num_layers,\n    num_heads=num_heads,\n    dropout=dropout\n)\nmodel = model.to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\nprint(f\"Model size: {total_params * 4 / (1024**2):.1f} MB\")\n\n\ncriterion = nn.CrossEntropyLoss(\n    ignore_index=vocab.stoi[\"<PAD>\"],\n    label_smoothing=label_smoothing\n)\noptimizer = AdamW(\n    model.parameters(),\n    lr=learning_rate,\n    weight_decay=weight_decay\n)\n\ntotal_steps = len(train_loader) * num_epochs\n#scheduler = CosineAnnealingWarmRestarts(\n#    optimizer,\n#    T_0=25,       # Önceki öneriyle aynı\n#    T_mult=1,     # Önceki öneriyle aynı\n#    eta_min=1e-6\n#)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',         \n    patience=2,       \n    factor=0.5,        \n    verbose=True,       \n    min_lr=1e-7        \n)\n\nbest_val_loss = float('inf')\ncheckpoint_dir = \"checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)\n\ntrain_losses = []\nval_losses = []\nlearning_rates = []\npatience = 10\ncounter = 0\n\nprint(\"Starting training...\")\nprint(f\"Total steps: {total_steps}\")\nprint(f\"Steps per epoch: {len(train_loader)}\")\nprint(\"=\" * 60)\n\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_train_loss = 0\n    epoch_start_time = time.time()\n\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} (Train)')\n\n    for idx, (imgs, captions) in enumerate(pbar):\n        imgs, captions = imgs.to(device), captions.to(device)\n\n        inputs = captions[:, :-1]\n        targets = captions[:, 1:]\n\n        model_output = model(imgs, inputs)\n\n        if isinstance(model_output, tuple):\n            outputs, attention_weights = model_output\n        else:\n            outputs = model_output\n\n        outputs = outputs.reshape(-1, outputs.shape[2])\n        targets = targets.reshape(-1)\n\n        loss = criterion(outputs, targets)\n        total_train_loss += loss.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n\n        optimizer.step()\n\n        pbar.set_postfix({\n            'Loss': f'{loss.item():.4f}',\n            'LR': f'{scheduler.get_last_lr()[0]:.2e}',\n            'Grad': f'{grad_norm:.2f}'\n        })\n\n        if idx % 100 == 0:\n            current_lr = scheduler.get_last_lr()[0]\n            learning_rates.append(current_lr) \n\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    model.eval() \n    total_val_loss = 0\n    val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} (Validation)')\n\n    with torch.no_grad(): \n        for val_idx, (val_imgs, val_captions) in enumerate(val_pbar):\n            val_imgs, val_captions = val_imgs.to(device), val_captions.to(device)\n\n            val_inputs = val_captions[:, :-1]\n            val_targets = val_captions[:, 1:]\n\n            val_model_output = model(val_imgs, val_inputs)\n\n            if isinstance(val_model_output, tuple):\n                val_outputs, _ = val_model_output\n            else:\n                val_outputs = val_model_output\n\n            val_outputs = val_outputs.reshape(-1, val_outputs.shape[2])\n            val_targets = val_targets.reshape(-1)\n\n            val_loss = criterion(val_outputs, val_targets)\n            total_val_loss += val_loss.item()\n            val_pbar.set_postfix({'Val Loss': f'{val_loss.item():.4f}'})\n\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_losses.append(avg_val_loss)\n\n    epoch_time = time.time() - epoch_start_time\n    scheduler.step(avg_val_loss)\n\n    print(f\"\\nEpoch [{epoch+1}/{num_epochs}] Summary:\")\n    print(f\"  Average Train Loss: {avg_train_loss:.4f}\")\n    print(f\"  Average Validation Loss: {avg_val_loss:.4f}\")\n    print(f\"  Time: {epoch_time:.1f}s\")\n    print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n\n\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        counter = 0 \n\n        checkpoint = {\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'best_val_loss': best_val_loss,\n            'vocab': vocab,\n            'hyperparameters': {\n                'embed_size': embed_size,\n                'hidden_size': hidden_size,\n                'num_heads': num_heads,\n                'num_layers': num_layers,\n                'dropout': dropout,\n                'learning_rate': learning_rate, \n                'batch_size': batch_size,\n                'num_epochs': num_epochs,\n                'max_norm': max_norm,\n                'weight_decay': weight_decay,\n                'label_smoothing': label_smoothing\n            }\n        }\n        torch.save(checkpoint, os.path.join(checkpoint_dir, \"best_model.pth\"))\n        print(f\"✅ Best model saved with VALIDATION loss: {best_val_loss:.4f}\")\n    else:\n        counter += 1\n        print(f\"Validation loss did not improve. Patience: {counter}/{patience}\")\n        if counter >= patience:\n            print(f\" Early stopping triggered after {epoch+1} epochs. Validation loss did not improve for {patience} consecutive epochs.\")\n            break \n\n    if (epoch + 1) % 5 == 0:\n        checkpoint = {\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'train_loss': avg_train_loss,\n            'val_loss': avg_val_loss,\n            'best_val_loss_so_far': best_val_loss, \n            'vocab': vocab,\n            'hyperparameters': {\n                'embed_size': embed_size,\n                'hidden_size': hidden_size,\n                'num_heads': num_heads,\n                'num_layers': num_layers,\n                'dropout': dropout,\n                'learning_rate': learning_rate,\n                'batch_size': batch_size,\n                'num_epochs': num_epochs,\n                'max_norm': max_norm,\n                'weight_decay': weight_decay,\n                'label_smoothing': label_smoothing\n            }\n        }\n        torch.save(checkpoint, os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pth\"))\n        print(f\"Checkpoint saved: epoch_{epoch+1} (Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f})\")\n\n    print(\"-\" * 60)\n\nprint(\"Training completed!\")\nprint(f\"Final best validation loss achieved: {best_val_loss:.4f}\")\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(learning_rates)\nplt.title('Learning Rate Schedule')\nplt.xlabel('Step ')\nplt.ylabel('Learning Rate')\nplt.yscale('log')\nplt.grid(True)\n\nplt.tight_layout()\nplt.savefig(os.path.join(checkpoint_dir, 'training_validation_curves.png'))\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:57:16.508515Z","iopub.execute_input":"2025-05-25T07:57:16.509035Z","iopub.status.idle":"2025-05-25T17:39:56.267009Z","shell.execute_reply.started":"2025-05-25T07:57:16.509013Z","shell.execute_reply":"2025-05-25T17:39:56.265361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport os \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ncheckpoint_path = \"/kaggle/working/checkpoints/best_model.pth\"\n\ntry:\n    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n\n  \n    hparams = checkpoint['hyperparameters']\n    vocab_size = len(checkpoint['vocab']) \n\n    model = ImageCaption(\n         embed_size=hparams['embed_size'],\n        hidden_size=hparams['hidden_size'],\n        vocab_size=vocab_size, \n        num_layers=hparams['num_layers'],\n        num_heads=hparams['num_heads'],\n        dropout = hparams['dropout']\n    )\n    model = model.to(device)\n\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval() \n\n   \n\n    vocab = checkpoint['vocab']\n    print(\"model yüklendi.\")\n\nexcept FileNotFoundError:\n    print(f\"Hata: Checkpoint dosyası bulunamadı: {checkpoint_path}\")\nexcept Exception as e:\n    print(f\"Checkpoint yüklenirken bir hata oluştu: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T13:57:36.112939Z","iopub.execute_input":"2025-05-24T13:57:36.113590Z","iopub.status.idle":"2025-05-24T13:57:40.512490Z","shell.execute_reply.started":"2025-05-24T13:57:36.113568Z","shell.execute_reply":"2025-05-24T13:57:40.511649Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Result","metadata":{}},{"cell_type":"code","source":"import pickle\n\ndef generate_caption(image_path, model, vocab, transform, max_length=26):\n    image = Image.open(image_path).convert(\"RGB\")\n    image = transform(image).unsqueeze(0).to(device)  \n\n    with torch.no_grad():\n        output ,_= model.caption_image(image, vocab, max_length=max_length)\n\n    caption = ' '.join(output)\n    return caption\n\ntransform = transforms.Compose([\n       transforms.Resize(256),        \n    transforms.CenterCrop(224),     \n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n\ntest_dir = '/kaggle/input/obss-intern-competition-2025/test/test'\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:40:22.859858Z","iopub.execute_input":"2025-05-25T17:40:22.860113Z","iopub.status.idle":"2025-05-25T17:50:55.847800Z","shell.execute_reply.started":"2025-05-25T17:40:22.860096Z","shell.execute_reply":"2025-05-25T17:50:55.847018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"captions = []\nfor img_id in test_captions[\"image_id\"]:\n    img_filename = f\"{img_id}.jpg\"  \n    img_path = os.path.join(test_dir, img_filename)\n\n    caption = generate_caption(img_path, model, vocab, transform)\n    cleaned_caption = ' '.join([\n        word for word in caption.split()\n        if word not in ['<UNK>', '<PAD>', '<SOS>', '<EOS>']\n    ])\n    captions.append(cleaned_caption)\n\ntest_captions[\"caption\"] = captions\n\n#test_captions.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:26:32.623367Z","iopub.execute_input":"2025-05-25T18:26:32.624333Z","iopub.status.idle":"2025-05-25T18:36:41.699996Z","shell.execute_reply.started":"2025-05-25T18:26:32.624299Z","shell.execute_reply":"2025-05-25T18:36:41.699360Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_images(test_captions.sample(15),test_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:38:38.668025Z","iopub.execute_input":"2025-05-25T18:38:38.668302Z","iopub.status.idle":"2025-05-25T18:38:40.104500Z","shell.execute_reply.started":"2025-05-25T18:38:38.668283Z","shell.execute_reply":"2025-05-25T18:38:40.103501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_captions.caption[6]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:35:23.022948Z","iopub.execute_input":"2025-05-25T19:35:23.023253Z","iopub.status.idle":"2025-05-25T19:35:23.028330Z","shell.execute_reply.started":"2025-05-25T19:35:23.023231Z","shell.execute_reply":"2025-05-25T19:35:23.027642Z"}},"outputs":[],"execution_count":null}]}